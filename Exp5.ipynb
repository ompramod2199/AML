{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Title - Reinforcement Learning\n",
        "\n",
        "a. Calculating Reward\n",
        "b. Discounted Reward\n",
        "c. Calculating Optimal quantities\n",
        "d. Implementing Q Learning\n",
        "e. Setting up an Optimal Action"
      ],
      "metadata": {
        "id": "ujQ202-1dCdb"
      },
      "id": "ujQ202-1dCdb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning (RL) is a type of machine learning paradigm where an agent learns to interact with an environment to achieve a specific goal. The agent receives feedback in the form of rewards or penalties based on its actions, guiding it towards maximizing the cumulative rewards over time. The code provided demonstrates various components of RL, including calculating rewards, implementing Q-learning, and finding optimal actions.\n",
        "\n",
        "a. Calculating Reward:\n",
        "In RL, the reward represents the immediate feedback given to the agent after each action. The reward_matrix in the code contains rewards for transitioning from one state to another. A reward of 1 indicates a positive outcome, while 0 denotes no reward. This matrix defines the immediate reward an agent receives when transitioning between states.\n",
        "\n",
        "b. Discounted Reward:\n",
        "The discounted reward is a fundamental concept in RL. It accounts for the fact that future rewards are less valuable than immediate rewards. The discount factor gamma (0.95 in this code) determines how much the agent values future rewards relative to immediate rewards. By using a discounted reward, the agent gives more importance to short-term gains while still considering the long-term cumulative rewards.\n",
        "\n",
        "c. Calculating Optimal Quantities:\n",
        "In RL, the agent aims to learn an optimal policy that maximizes the cumulative rewards. The Q-matrix (quality matrix) stores the estimated value (Q-value) of taking specific actions from each state. The Q-value represents the expected cumulative reward when taking a particular action in a given state and following the optimal policy thereafter.\n",
        "\n",
        "d. Implementing Q Learning:\n",
        "The q_learning_update function implements the Q-learning algorithm, a popular model-free RL method. It updates the Q-matrix based on the observed rewards and transitions experienced during exploration of the environment. The agent learns to estimate the Q-values iteratively by updating them using a learning rate (alpha) and the discounted reward for the next state.\n",
        "\n",
        "e. Setting up an Optimal Action:\n",
        "The function find_optimal_route demonstrates how the agent uses Q-learning to find an optimal route from the initial state 'A' to the goal state 'E'. The agent explores the environment by taking actions and updating the Q-matrix until it reaches the goal state. Over time, the Q-matrix converges to the optimal Q-values, representing the optimal policy for each state.\n",
        "\n",
        "In conclusion, Reinforcement Learning is a powerful technique for training agents to learn from their interactions with an environment. The provided code showcases some fundamental aspects of RL, such as calculating rewards, implementing Q-learning, and finding optimal actions. By combining exploration and exploitation, RL algorithms can discover optimal policies and make effective decisions in various real-world scenarios, from game playing to robotics and autonomous systems. The ability to learn from feedback and adapt to changing environments makes RL an essential tool for solving complex decision-making problems where traditional supervised learning approaches may not be applicable."
      ],
      "metadata": {
        "id": "tmSz8b5Vc-mc"
      },
      "id": "tmSz8b5Vc-mc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5992de17",
      "metadata": {
        "id": "5992de17"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "state_to_index = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
        "index_to_state = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
        "\n",
        "reward_matrix = np.array([\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [1, 0, 1, 0, 0],\n",
        "    [0, 1, 0, 1, 0],\n",
        "    [0, 0, 1, 0, 1],\n",
        "    [0, 0, 0, 1, 0]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16b83b9",
      "metadata": {
        "id": "b16b83b9",
        "outputId": "253d16df-3141-463a-f9d6-537fea0266d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward Matrix:\n",
            "[[0 1 0 0 0]\n",
            " [1 0 1 0 0]\n",
            " [0 1 0 1 0]\n",
            " [0 0 1 0 1]\n",
            " [0 0 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Reward Matrix:\")\n",
        "print(reward_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2b21b3",
      "metadata": {
        "id": "0a2b21b3"
      },
      "outputs": [],
      "source": [
        "gamma = 0.95  # Discount factor\n",
        "alpha = 0.1  # Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8e535f",
      "metadata": {
        "id": "9a8e535f",
        "outputId": "43d93736-adf0-4117-bdf8-901e3e1c0b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discount Factor (Gamma): 0.95\n"
          ]
        }
      ],
      "source": [
        "print(\"Discount Factor (Gamma):\", gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a800ca",
      "metadata": {
        "id": "96a800ca"
      },
      "outputs": [],
      "source": [
        "state_size = len(state_to_index)\n",
        "action_size = state_size\n",
        "Q_matrix = np.zeros([state_size, action_size])\n",
        "\n",
        "def q_learning_update(s, a, reward, s2, Q_matrix):\n",
        "    Q_matrix[s, a] = (1 - alpha) * Q_matrix[s, a] + alpha * (reward + gamma * np.max(Q_matrix[s2, :]))\n",
        "    s = s2\n",
        "    return s, Q_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7471110a",
      "metadata": {
        "id": "7471110a"
      },
      "outputs": [],
      "source": [
        "def get_action(state, Q_matrix, epsilon=0.1):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.choice(action_size)\n",
        "    return np.argmax(Q_matrix[state, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb258c10",
      "metadata": {
        "id": "cb258c10"
      },
      "outputs": [],
      "source": [
        "def find_optimal_route(initial_state, goal_state, Q_matrix, episodes=1000):\n",
        "    for _ in range(episodes):\n",
        "        state = initial_state\n",
        "        while state != goal_state:\n",
        "            action = get_action(state, Q_matrix)\n",
        "            next_state = action\n",
        "            reward = reward_matrix[state, action]\n",
        "            state, Q_matrix = q_learning_update(state, action, reward, next_state, Q_matrix)\n",
        "    return Q_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e6cd1e",
      "metadata": {
        "id": "80e6cd1e",
        "outputId": "0ff853b0-f9ce-4504-8d85-2358cfe7c1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-matrix:\n",
            "[[19.         20.         18.05       18.05        0.        ]\n",
            " [20.         19.         19.05       18.05        0.        ]\n",
            " [19.         16.66329168 15.68551912 17.49748427  0.        ]\n",
            " [19.         15.82919946 15.84060089 16.06107254  0.89058101]\n",
            " [ 0.          0.          0.          0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "initial_state = state_to_index['A']\n",
        "goal_state = state_to_index['E']\n",
        "Q_matrix = find_optimal_route(initial_state, goal_state, Q_matrix)\n",
        "\n",
        "print(\"Q-matrix:\")\n",
        "print(Q_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abce454f",
      "metadata": {
        "id": "abce454f",
        "outputId": "e456b1a0-a900-44a7-834a-a2fc8cac4433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Actions for each state: ['B', 'A', 'A', 'A', 'A']\n"
          ]
        }
      ],
      "source": [
        "optimal_actions = [np.argmax(Q_matrix[state, :]) for state in range(state_size)]\n",
        "optimal_actions = [index_to_state[action] for action in optimal_actions]\n",
        "print(\"Optimal Actions for each state:\", optimal_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d300b9",
      "metadata": {
        "id": "61d300b9",
        "outputId": "66215a1c-ea90-4277-ef1a-260d6fa6eae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Immediate Reward for moving from state A to state A: 0\n",
            "Immediate Reward for moving from state A to state B: 1\n",
            "Immediate Reward for moving from state A to state C: 0\n",
            "Immediate Reward for moving from state A to state D: 0\n",
            "Immediate Reward for moving from state A to state E: 0\n",
            "Immediate Reward for moving from state B to state A: 1\n",
            "Immediate Reward for moving from state B to state B: 0\n",
            "Immediate Reward for moving from state B to state C: 1\n",
            "Immediate Reward for moving from state B to state D: 0\n",
            "Immediate Reward for moving from state B to state E: 0\n",
            "Immediate Reward for moving from state C to state A: 0\n",
            "Immediate Reward for moving from state C to state B: 1\n",
            "Immediate Reward for moving from state C to state C: 0\n",
            "Immediate Reward for moving from state C to state D: 1\n",
            "Immediate Reward for moving from state C to state E: 0\n",
            "Immediate Reward for moving from state D to state A: 0\n",
            "Immediate Reward for moving from state D to state B: 0\n",
            "Immediate Reward for moving from state D to state C: 1\n",
            "Immediate Reward for moving from state D to state D: 0\n",
            "Immediate Reward for moving from state D to state E: 1\n",
            "Immediate Reward for moving from state E to state A: 0\n",
            "Immediate Reward for moving from state E to state B: 0\n",
            "Immediate Reward for moving from state E to state C: 0\n",
            "Immediate Reward for moving from state E to state D: 1\n",
            "Immediate Reward for moving from state E to state E: 0\n"
          ]
        }
      ],
      "source": [
        "for state in state_to_index:\n",
        "    for action in state_to_index:\n",
        "        state_idx = state_to_index[state]\n",
        "        action_idx = state_to_index[action]\n",
        "        immediate_reward = reward_matrix[state_idx, action_idx]\n",
        "        print(f\"Immediate Reward for moving from state {state} to state {action}: {immediate_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53600d81",
      "metadata": {
        "id": "53600d81",
        "outputId": "95b30a89-f047-497f-bd04-01d91a0d4473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discounted Reward for the action sequence: 3.709875\n"
          ]
        }
      ],
      "source": [
        "action_sequence = ['A', 'B', 'C', 'D', 'E']\n",
        "discounted_reward = 0\n",
        "current_gamma = 1\n",
        "\n",
        "for i in range(len(action_sequence) - 1):\n",
        "    state = state_to_index[action_sequence[i]]\n",
        "    next_state = state_to_index[action_sequence[i+1]]\n",
        "    reward = reward_matrix[state, next_state]\n",
        "    discounted_reward += current_gamma * reward\n",
        "    current_gamma *= gamma\n",
        "\n",
        "print(\"Discounted Reward for the action sequence:\", discounted_reward)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}